import logger from '../utils/logger.js';
import * as domainDataRepository from '../database/repositories/domainDataRepository.js';
import * as discoveryService from './discoveryService.js';
import * as analysisService from './analysisService.js';
import * as brandfetchService from './brandfetchService.js';
import * as aiAnalysisService from './aiAnalysisService.js';
import io from '../utils/io.js';
import * as puppeteerService from './puppeteerService.js';

// Import content extractors
import * as generalExtractor from './contentExtractors/generalExtractor.js';
import * as blogExtractor from './contentExtractors/blogExtractor.js';
import * as imageExtractor from './contentExtractors/imageExtractor.js';
import * as enhancedImageExtractor from './contentExtractors/enhancedImageExtractor.js';
import * as colorExtractor from './contentExtractors/colorExtractor.js';
import * as socialMediaExtractor from './contentExtractors/socialMediaExtractor.js';
import * as videoExtractor from './contentExtractors/videoExtractor.js';
import * as isbnExtractor from './contentExtractors/isbnExtractor.js';
import * as appExtractor from './contentExtractors/appExtractor.js';
import * as rssExtractor from './contentExtractors/rssExtractor.js';
import * as podcastExtractor from './contentExtractors/podcastExtractor.js';

// In-memory job queue and active jobs
const jobQueue = {
  high: [],
  normal: [],
  low: []
};

const activeJobs = new Map();
const completedJobs = new Map();
const MAX_COMPLETED_JOBS = 100; // Maximum number of completed jobs to keep in memory
const MAX_CONCURRENT_JOBS = process.env.MAX_CONCURRENT_JOBS ? parseInt(process.env.MAX_CONCURRENT_JOBS) : 5;

// Initialize the scrape manager
export const init = async () => {
  logger.info('Initializing scrape manager...');
  
  // Load any pending jobs from the database
  try {
    const pendingJobs = await domainDataRepository.getPendingJobs();
    
    if (pendingJobs && pendingJobs.length > 0) {
      logger.info(`Found ${pendingJobs.length} pending jobs to resume`);
      
      pendingJobs.forEach(job => {
        if (job.status === 'processing') {
          // Reset to queued for jobs that were processing when the service stopped
          job.status = 'queued';
        }
        
        // Ensure job.priority exists and is valid
        const priority = job.priority && ['high', 'normal', 'low'].includes(job.priority) 
          ? job.priority 
          : 'normal';
        
        jobQueue[priority].push(job);
      });
    }
    
    // Start the job processor
    processNextJob();
    return true;
  } catch (error) {
    logger.error(`Error initializing scrape manager: ${error.message}`);
    // Continue without database jobs
    processNextJob();
    return false;
  }
};

// Helper function to emit socket events safely
const emitSocketEvent = (room, event, data) => {
  try {
    if (global.io) {
      global.io.to(room).emit(event, data);
    }
  } catch (error) {
    logger.error(`Error emitting socket event: ${error.message}`);
  }
};

// Queue a new job
export const queueJob = async (job) => {
  try {
    // Save job to database
    await domainDataRepository.saveJob(job);
    
    // Add to in-memory queue
    jobQueue[job.priority].push(job);
    
    // Process next job if not at capacity
    if (activeJobs.size < MAX_CONCURRENT_JOBS) {
      processNextJob();
    }
    
    return {
      jobId: job.jobId,
      status: job.status,
      estimatedTime: '30s'
    };
  } catch (error) {
    logger.error(`Error queueing job: ${error.message}`);
    throw new Error(`Failed to queue job: ${error.message}`);
  }
};

// Process the next job in the queue
const processNextJob = async () => {
  // If at capacity, don't process more jobs
  if (activeJobs.size >= MAX_CONCURRENT_JOBS) {
    return;
  }
  
  // Find the next job to process (high priority first)
  let nextJob = null;
  
  if (jobQueue.high.length > 0) {
    nextJob = jobQueue.high.shift();
  } else if (jobQueue.normal.length > 0) {
    nextJob = jobQueue.normal.shift();
  } else if (jobQueue.low.length > 0) {
    nextJob = jobQueue.low.shift();
  }
  
  if (nextJob) {
    // Process the job
    processJob(nextJob);
    
    // Check if we can process more jobs
    if (activeJobs.size < MAX_CONCURRENT_JOBS) {
      processNextJob();
    }
  }
};

// Process a job
const processJob = async (job) => {
  try {
    // Update job status to processing
    job.status = 'processing';
    job.startedAt = new Date().toISOString();
    
    // Add to active jobs
    activeJobs.set(job.jobId, job);
    
    // Update job status in database
    try {
      await domainDataRepository.updateJobStatus(job.jobId, 'processing', job.startedAt);
    } catch (error) {
      logger.error(`Error updating job status: ${error.message}`);
    }
    
    logger.info(`[JOB] üöÄ Starting job ${job.jobId} for domain ${job.domain}`);
    
    // Check if this is a resume of a previous job
    const canResume = await domainDataRepository.canResumeJob(job.jobId);
    if (canResume) {
      logger.info(`[JOB] üîÑ Resuming previous crawl for job ${job.jobId}`);
    }
    
    // Update progress
    job.progress = 10;
    job.message = 'Discovering pages';
    
    // Notify clients of progress update
    emitSocketEvent(`job-${job.jobId}`, 'job-update', {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: job.message
    });
    
    // Get domain info ID - will need this for saving data
    let domainInfoId = null;
    try {
      // Get or create domain_info entry
      const [existingDomainInfo] = await domainDataRepository.query(
        'SELECT id FROM domain_info WHERE domain = ?',
        [job.domain]
      );
      
      if (existingDomainInfo && existingDomainInfo.length > 0) {
        domainInfoId = existingDomainInfo[0].id;
      } else {
        const [result] = await domainDataRepository.query(
          'INSERT INTO domain_info (domain, status, created_at) VALUES (?, "processing", NOW())',
          [job.domain]
        );
        domainInfoId = result.insertId;
      }
      
      logger.info(`[JOB] Using domain ID: ${domainInfoId} for ${job.domain}`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error creating domain_info entry: ${error.message}`);
    }
    
    // Discover pages
    const pages = await discoveryService.discoverPages(job.domain, job.depth, job.jobId, {
      maxPages: job.maxPages || 25, // Use job-specific limit if provided
      bypassCooldown: true, // Bypass the 24-hour cooldown
      cooldownMinutes: 5 // If not bypassing completely, use a very short cooldown period
    });
    logger.info(`[JOB] üìã Discovered ${pages.length} pages for ${job.domain}`);
    
    // Check if any pages were discovered
    if (!pages || pages.length === 0) {
      logger.warn(`[JOB] ‚ö†Ô∏è No pages discovered for ${job.domain}. Creating minimal results.`);
      
      // Create minimal results with empty data
      const minimalResults = {
        domain: job.domain,
        scrapedAt: new Date().toISOString(),
        general: {
          siteStructure: {
            title: `${job.domain} Website`,
            meta: {
              description: `Website for ${job.domain}`,
              keywords: job.domain
            },
            sections: []
          },
          prominentLinks: [],
          navigationStructure: {
            mainNav: [],
            footerNav: []
          }
        },
        blog: {
          hasBlog: false,
          blogUrl: null,
          articles: []
        },
        images: {
          all: [],
          byCategory: {},
          heroImages: [],
          brandImages: [],
          productImages: [],
          contentImages: [],
          backgroundImages: [],
          bannerImages: [],
          galleryImages: [],
          socialProofImages: [],
          teamImages: [],
          otherImages: []
        },
        colors: {
          primaryColor: '#4285f4',
          secondaryColors: ['#ea4335', '#fbbc05', '#34a853'],
          palette: ['#4285f4', '#ea4335', '#fbbc05', '#34a853', '#ffffff', '#000000']
        },
        isbn: {
          isbns: [],
          isbnImages: []
        }
      };
      
      // Skip to saving minimal results
      job.progress = 90;
      job.message = 'Saving minimal results';
      
      // Notify clients of progress update
      emitSocketEvent(`job-${job.jobId}`, 'job-update', {
        jobId: job.jobId,
        status: job.status,
        progress: job.progress,
        message: job.message
      });
      
      try {
        await domainDataRepository.saveResults(job.jobId, minimalResults);
        logger.info(`[JOB] Saved minimal results for job ${job.jobId}`);
        
        // Complete the job
        job.status = 'complete';
        job.progress = 100;
        job.message = 'Scrape completed with minimal results';
        job.completedAt = new Date().toISOString();
        
        // Update job status in database
        await domainDataRepository.updateJobStatus(job.jobId, 'complete', job.startedAt, job.completedAt);
        
        // Notify clients of job completion
        emitSocketEvent(`job-${job.jobId}`, 'job-update', {
          jobId: job.jobId,
          status: job.status,
          progress: job.progress,
          message: job.message
        });
        
        // Remove from active jobs and add to completed jobs
        activeJobs.delete(job.jobId);
        completedJobs.set(job.jobId, job);
        
        // Process next job if available
        return processNextJob();
      } catch (error) {
        logger.error(`[JOB] ‚ùå Error saving minimal results: ${error.message}`);
        throw new Error(`Failed to save minimal results: ${error.message}`);
      }
    }
    
    // Update progress
    job.progress = 30;
    job.message = 'Extracting content';
    
    // Notify clients of progress update
    emitSocketEvent(`job-${job.jobId}`, 'job-update', {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: job.message
    });
    
    // Extract content from pages
    const pageContents = [];
    for (let i = 0; i < pages.length; i++) {
      const page = pages[i];
      logger.info(`[JOB] Extracting content from page ${i+1}/${pages.length}: ${page.url}`);
      const content = await discoveryService.extractPageContent(page.url);
      pageContents.push(content);
      
      // Update progress incrementally
      job.progress = Math.min(30 + Math.floor((pageContents.length / pages.length) * 30), 60);
      
      // Notify clients of progress update
      emitSocketEvent(`job-${job.jobId}`, 'job-update', {
        jobId: job.jobId,
        status: job.status,
        progress: job.progress,
        message: `Extracted content from ${pageContents.length} of ${pages.length} pages`
      });
    }
    
    // Update progress
    job.progress = 60;
    job.message = 'Extracting detailed information';
    
    // Notify clients of progress update
    emitSocketEvent(`job-${job.jobId}`, 'job-update', {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: job.message
    });
    
    // Convert page content to format expected by extractors
    const pagesWithContent = pages.map((page, index) => ({
      url: page.url,
      title: page.title,
      content: pageContents[index]?.content || ''
    }));
    
    // Process all extractors and save directly to database
    logger.info(`[JOB] Running content extractors for ${job.domain}`);
    
    // Extract enhanced image data
    // Note: Images are now saved to database directly by the extractor
    let enhancedImageData = null;
    try {
      logger.info(`[JOB] Running enhanced image extractor`);
      enhancedImageData = await enhancedImageExtractor.extract(pagesWithContent, {
        domainId: domainInfoId,
        saveToDatabase: true
      });
      logger.info(`[JOB] ‚úÖ Image extraction complete. Found ${enhancedImageData.all.length} images categorized into ${Object.keys(enhancedImageData.byCategory).length} categories`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting enhanced image data: ${error.message}`);
      logger.error(error.stack);
    }
    
    // Extract social media data
    // Note: Social links are now saved to database directly by the extractor
    let socialMediaData = null;
    try {
      logger.info(`[JOB] Running social media extractor`);
      socialMediaData = await socialMediaExtractor.extract(pagesWithContent);
      logger.info(`[JOB] ‚úÖ Social media extraction complete. Found ${Object.keys(socialMediaData.links).length} social platforms`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting social media data: ${error.message}`);
    }
    
    // Extract blog content
    let blogData = null;
    try {
      logger.info(`[JOB] Running blog extractor`);
      blogData = await blogExtractor.extract(pagesWithContent);
      logger.info(`[JOB] ‚úÖ Blog extraction complete. ${blogData.hasBlog ? `Found blog at ${blogData.blogUrl} with ${blogData.articles.length} articles` : 'No blog found'}`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting blog data: ${error.message}`);
    }
    
    // Extract colors
    let colorData = null;
    try {
      logger.info(`[JOB] Running color extractor`);
      colorData = await colorExtractor.extract(pagesWithContent);
      logger.info(`[JOB] ‚úÖ Color extraction complete. Found primary color ${colorData.primaryColor} and ${colorData.secondaryColors.length} secondary colors`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting color data: ${error.message}`);
    }
    
    // Extract general website information
    let generalData = null;
    try {
      logger.info(`[JOB] Running general extractor`);
      generalData = await generalExtractor.extract(pagesWithContent);
      logger.info(`[JOB] ‚úÖ General extraction complete`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting general data: ${error.message}`);
    }
    
    // Extract ISBN data
    let isbnData = null;
    try {
      logger.info(`[JOB] Running ISBN extractor`);
      isbnData = await isbnExtractor.extract(pagesWithContent);
      logger.info(`[JOB] ‚úÖ ISBN extraction complete. Found ${isbnData.isbns.length} ISBN numbers and ${isbnData.isbnImages.length} related images`);
    } catch (error) {
      logger.error(`[JOB] ‚ùå Error extracting ISBN data: ${error.message}`);
    }
    
    // Update progress
    job.progress = 80;
    job.message = 'Building final results';
    
    // Notify clients of progress update
    emitSocketEvent(`job-${job.jobId}`, 'job-update', {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: job.message
    });
    
    // Combine all results
    const results = {
      domain: job.domain,
      scrapedAt: new Date().toISOString(),
      pageCount: pages.length,
      general: generalData || {
        siteStructure: {
          title: job.domain,
          meta: {},
          sections: []
        },
        prominentLinks: [],
        navigationStructure: {
          mainNav: [],
          footerNav: []
        }
      },
      blog: blogData || {
        hasBlog: false,
        blogUrl: null,
        articles: []
      },
      images: enhancedImageData || {
        all: [],
        byCategory: {},
        heroImages: [],
        brandImages: []
      },
      colors: colorData || {
        primaryColor: '#4285f4',
        secondaryColors: ['#ea4335', '#fbbc05', '#34a853'],
        palette: ['#4285f4', '#ea4335', '#fbbc05', '#34a853', '#ffffff', '#000000']
      },
      socialMedia: socialMediaData || {
        links: [],
        content: {}
      },
      isbn: isbnData || {
        isbns: [],
        isbnImages: []
      }
    };
    
    // Store the website data in domain_info
    if (domainInfoId) {
      try {
        await domainDataRepository.query(
          'UPDATE domain_info SET data = ? WHERE id = ?',
          [JSON.stringify(results), domainInfoId]
        );
        
        logger.info(`[JOB] ‚úÖ Stored website data in domain_info for ${job.domain}`);
      } catch (error) {
        logger.error(`[JOB] ‚ùå Error storing website data: ${error.message}`);
      }
    }
    
    // Complete the job
    job.status = 'complete';
    job.progress = 100;
    job.message = 'Scrape completed';
    job.completedAt = new Date().toISOString();
    
    // Update job status in database
    await domainDataRepository.updateJobStatus(job.jobId, 'complete', job.startedAt, job.completedAt);
    
    // Notify clients of job completion
    emitSocketEvent(`job-${job.jobId}`, 'job-update', {
      jobId: job.jobId,
      status: job.status,
      progress: job.progress,
      message: job.message
    });
    
    // Remove from active jobs and add to completed jobs
    activeJobs.delete(job.jobId);
    completedJobs.set(job.jobId, job);
    
    // Process next job if available
    return processNextJob();
  } catch (error) {
    logger.error(`[JOB] ‚ùå Error processing job: ${error.message}`);
    throw new Error(`Failed to process job: ${error.message}`);
  }
};

// Get job status
export const getJobStatus = async (jobId) => {
  try {
    // Check in-memory jobs first
    if (activeJobs.has(jobId)) {
      return activeJobs.get(jobId);
    }
    
    if (completedJobs.has(jobId)) {
      return completedJobs.get(jobId);
    }
    
    // Check database
    const jobStatus = await domainDataRepository.getJobStatus(jobId);
    return jobStatus;
  } catch (error) {
    logger.error(`Error getting job status: ${error.message}`);
    throw new Error(`Failed to get job status: ${error.message}`);
  }
};

// Cancel a job
export const cancelJob = async (jobId) => {
  try {
    // Check if job is active
    if (activeJobs.has(jobId)) {
      const job = activeJobs.get(jobId);
      
      // Update job status
      job.status = 'cancelled';
      job.progress = 0;
      job.message = 'Job cancelled by user';
      job.completedAt = new Date().toISOString();
      
      // Update job status in database
      await domainDataRepository.updateJobStatus(job.jobId, 'cancelled', job.startedAt, job.completedAt);
      
      // Remove from active jobs and add to completed jobs
      activeJobs.delete(jobId);
      completedJobs.set(jobId, job);
      
      // Process next job if available
      processNextJob();
      
      return { success: true, message: 'Job cancelled successfully' };
    }
    
    // Check if job is in queue
    for (const priority of ['high', 'normal', 'low']) {
      const index = jobQueue[priority].findIndex(job => job.jobId === jobId);
      
      if (index !== -1) {
        const job = jobQueue[priority][index];
        
        // Remove from queue
        jobQueue[priority].splice(index, 1);
        
        // Update job status in database
        await domainDataRepository.updateJobStatus(job.jobId, 'cancelled', null, new Date().toISOString());
        
        return { success: true, message: 'Job cancelled successfully' };
      }
    }
    
    // Job not found in memory, try to cancel in database
    const result = await domainDataRepository.updateJobStatus(jobId, 'cancelled', null, new Date().toISOString());
    
    if (result) {
      return { success: true, message: 'Job cancelled successfully' };
    } else {
      return { success: false, message: 'Job not found' };
    }
  } catch (error) {
    logger.error(`Error cancelling job: ${error.message}`);
    return { success: false, message: `Error cancelling job: ${error.message}` };
  }
};

// List all jobs
export const listJobs = async (status, limit, offset) => {
  try {
    // Get jobs from database
    const jobs = await domainDataRepository.listJobs(status, limit, offset);
    return jobs;
  } catch (error) {
    logger.error(`Error listing jobs: ${error.message}`);
    throw new Error(`Failed to list jobs: ${error.message}`);
  }
};

// Extract content using specified extractors
const extractContent = async (domain, pages, extractors) => {
  const results = {
    domain
  };
  
  try {
    // Extract general information
    if (extractors.includes('general')) {
      results.general = await generalExtractor.extract(pages);
    }
    
    // Extract blog content
    if (extractors.includes('blog')) {
      results.blog = await blogExtractor.extract(pages);
    }
    
    // Extract images
    if (extractors.includes('images')) {
      results.images = await enhancedImageExtractor.extract(pages);
    }
    
    // Extract colors
    if (extractors.includes('colors')) {
      results.colors = await colorExtractor.extract(pages);
    }
    
    // Extract social media
    if (extractors.includes('social')) {
      results.socialMedia = await socialMediaExtractor.extract(pages);
    }
    
    // Extract videos
    if (extractors.includes('videos')) {
      results.videos = await videoExtractor.extract(pages);
    }
    
    // Extract ISBN data
    if (extractors.includes('isbn')) {
      const isbnData = await isbnExtractor.extract(pages);
      results.isbnData = isbnData.isbns;
      results.isbnImages = isbnData.images;
    }
    
    // Extract app links (new)
    if (extractors.includes('apps')) {
      results.apps = await appExtractor.extract(pages);
    }
    
    // Extract RSS and podcast feeds (new)
    if (extractors.includes('rss') || extractors.includes('podcast')) {
      results.podcastInfo = await podcastExtractor.extract(pages);
    }
    
    return results;
  } catch (error) {
    logger.error(`Error extracting content: ${error.message}`);
    throw error;
  }
};